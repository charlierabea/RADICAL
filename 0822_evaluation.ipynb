{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studywise\n",
    "import pandas as pd\n",
    "\n",
    "def extract_labels_from_scores(row, labels, threshold=0.5):\n",
    "    return [label for label in labels if row[label] > threshold]\n",
    "\n",
    "def calculate_metrics(predictions, ground_truth, labels):\n",
    "    TP = sum([1 for label in labels if label in predictions and label in ground_truth])\n",
    "    TN = sum([1 for label in labels if label not in predictions and label not in ground_truth])\n",
    "    FP = sum([1 for label in labels if label in predictions and label not in ground_truth])\n",
    "    FN = sum([1 for label in labels if label not in predictions and label in ground_truth])\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) != 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    return {\"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1_score}\n",
    "\n",
    "def main():\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(\"/Users/charliethebear/Documents/Lab/2023_summer/excel/0822_average_scores.xlsx\")\n",
    "\n",
    "    # Extract the list of labels from the column names\n",
    "    labels = list(df.columns)[1:19] # Assuming the first column is 'Image Name' and the last two columns are 'Extracted labels' and 'gt labels'\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    # Process the data\n",
    "    for _, row in df.iterrows():\n",
    "        gt_labels = row['gt_labels'].split(', ')\n",
    "        biomedclip_labels = extract_labels_from_scores(row, labels)\n",
    "        gpt_labels = row['Extracted_labels(GPT-3.5)'].split(', ')\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics_biomedclip_vs_gt = calculate_metrics(biomedclip_labels, gt_labels, labels)\n",
    "        metrics_gpt_vs_gt = calculate_metrics(gpt_labels, gt_labels, labels)\n",
    "        metrics_biomedclip_vs_gpt = calculate_metrics(biomedclip_labels, gpt_labels, labels)\n",
    "\n",
    "        metrics_data.append({\n",
    "            \"Image Name\": row[\"Image Name\"],\n",
    "            **{f\"BiomedCLIP vs GT - {key}\": value for key, value in metrics_biomedclip_vs_gt.items()},\n",
    "            **{f\"GPT vs GT - {key}\": value for key, value in metrics_gpt_vs_gt.items()},\n",
    "            **{f\"BiomedCLIP vs GPT - {key}\": value for key, value in metrics_biomedclip_vs_gpt.items()}\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df.to_excel(\"/Users/charliethebear/Documents/Lab/2023_summer/excel/0822_evaluation.xlsx\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelwise\n",
    "import pandas as pd\n",
    "\n",
    "def extract_labels_from_scores(row, labels, threshold=0.5):\n",
    "    return [label for label in labels if row[label] > threshold]\n",
    "\n",
    "def calculate_labelwise_metrics(df, labels):\n",
    "    labelwise_data = []\n",
    "\n",
    "    for label in labels:\n",
    "        metrics_data = {\"Label\": label}\n",
    "\n",
    "        for comparison, prefix in [(\"BiomedCLIP vs GT\", \"BiomedCLIP vs GT\"), (\"GPT vs GT\", \"GPT vs GT\"), (\"BiomedCLIP vs GPT\", \"BiomedCLIP vs GPT\")]:\n",
    "            tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                gt_labels = row['gt_labels'].split(', ')\n",
    "                biomedclip_labels = extract_labels_from_scores(row, labels)\n",
    "                gpt_labels = row['Extracted_labels(GPT-3.5)'].split(', ')\n",
    "\n",
    "                pred_labels = biomedclip_labels if \"BiomedCLIP\" in comparison else gpt_labels\n",
    "                true_labels = gt_labels if \"GT\" in comparison else (gpt_labels if \"GPT\" in comparison and \"BiomedCLIP\" not in comparison else biomedclip_labels)\n",
    "\n",
    "                if label in pred_labels and label in true_labels: tp += 1\n",
    "                if label not in pred_labels and label not in true_labels: tn += 1\n",
    "                if label in pred_labels and label not in true_labels: fp += 1\n",
    "                if label not in pred_labels and label in true_labels: fn += 1\n",
    "            \n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) != 0 else 0\n",
    "            precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            metrics_data.update({\n",
    "                f\"{prefix} - TP\": tp, f\"{prefix} - TN\": tn, f\"{prefix} - FP\": fp, f\"{prefix} - FN\": fn,\n",
    "                f\"{prefix} - Accuracy\": accuracy, f\"{prefix} - Precision\": precision, f\"{prefix} - Recall\": recall, f\"{prefix} - F1 Score\": f1_score\n",
    "            })\n",
    "\n",
    "        labelwise_data.append(metrics_data)\n",
    "\n",
    "    return pd.DataFrame(labelwise_data)\n",
    "\n",
    "def main():\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(\"/Users/charliethebear/Documents/Lab/2023_summer/excel/0822_average_scores.xlsx\")\n",
    "\n",
    "    # Extract the list of labels from the column names\n",
    "    labels = list(df.columns)[1:19]  # Adjust this according to the column names in your Excel file\n",
    "\n",
    "    # Calculate label-wise metrics\n",
    "    labelwise_df = calculate_labelwise_metrics(df, labels)\n",
    "\n",
    "    # Write the results to an Excel file\n",
    "    labelwise_df.to_excel(\"/Users/charliethebear/Documents/Lab/2023_summer/excel/0822_evaluation_labelwise.xlsx\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
